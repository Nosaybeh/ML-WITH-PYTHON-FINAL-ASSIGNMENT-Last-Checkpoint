import itertools
2
import numpy as np
3
import matplotlib.pyplot as plt
4
from matplotlib.ticker import NullFormatter
5
import pandas as pd
6
import numpy as np
7
import matplotlib.ticker as ticker
8
from sklearn import preprocessing
1
df = pd.read_csv('dataset\loan_train.csv')
2
#data = pd.read_csv('dataset\pima.csv')
3
df.head()
Unnamed: 0	Unnamed: 0.1	loan_status	Principal	terms	effective_date	due_date	age	education	Gender
0	0	0	PAIDOFF	1000	30	9/8/2016	10/7/2016	45	High School or Below	male
1	2	2	PAIDOFF	1000	30	9/8/2016	10/7/2016	33	Bechalor	female
2	3	3	PAIDOFF	1000	15	9/8/2016	9/22/2016	27	college	male
3	4	4	PAIDOFF	1000	30	9/9/2016	10/8/2016	28	college	female
4	6	6	PAIDOFF	1000	30	9/9/2016	10/8/2016	29	college	male
1
df.shape
(346, 10)
1
df['due_date'] = pd.to_datetime(df['due_date'])
2
df['effective_date'] = pd.to_datetime(df['effective_date'])
3
df.head()
Unnamed: 0	Unnamed: 0.1	loan_status	Principal	terms	effective_date	due_date	age	education	Gender
0	0	0	PAIDOFF	1000	30	2016-09-08	2016-10-07	45	High School or Below	male
1	2	2	PAIDOFF	1000	30	2016-09-08	2016-10-07	33	Bechalor	female
2	3	3	PAIDOFF	1000	15	2016-09-08	2016-09-22	27	college	male
3	4	4	PAIDOFF	1000	30	2016-09-09	2016-10-08	28	college	female
4	6	6	PAIDOFF	1000	30	2016-09-09	2016-10-08	29	college	male
1
df['loan_status'].value_counts()
PAIDOFF       260
COLLECTION     86
Name: loan_status, dtype: int64
1
import seaborn as sns
2
​
3
bins = np.linspace(df.Principal.min(), df.Principal.max(), 10)
4
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
5
g.map(plt.hist, 'Principal', bins=bins, ec="k")
6
​
7
g.axes[-1].legend()
8
plt.show()

1
#import sys
2
#print(sys.path)
1
bins = np.linspace(df.age.min(), df.age.max(), 10)
2
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
3
g.map(plt.hist, 'age', bins=bins, ec="k")
4
​
5
g.axes[-1].legend()
6
plt.show()

1
df['dayofweek'] = df['effective_date'].dt.dayofweek
2
bins = np.linspace(df.dayofweek.min(), df.dayofweek.max(), 10)
3
g = sns.FacetGrid(df, col="Gender", hue="loan_status", palette="Set1", col_wrap=2)
4
g.map(plt.hist, 'dayofweek', bins=bins, ec="k")
5
g.axes[-1].legend()
6
plt.show()

1
df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)
2
df.head()
Unnamed: 0	Unnamed: 0.1	loan_status	Principal	terms	effective_date	due_date	age	education	Gender	dayofweek	weekend
0	0	0	PAIDOFF	1000	30	2016-09-08	2016-10-07	45	High School or Below	male	3	0
1	2	2	PAIDOFF	1000	30	2016-09-08	2016-10-07	33	Bechalor	female	3	0
2	3	3	PAIDOFF	1000	15	2016-09-08	2016-09-22	27	college	male	3	0
3	4	4	PAIDOFF	1000	30	2016-09-09	2016-10-08	28	college	female	4	1
4	6	6	PAIDOFF	1000	30	2016-09-09	2016-10-08	29	college	male	4	1
1
df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
Gender  loan_status
female  PAIDOFF        0.865385
        COLLECTION     0.134615
male    PAIDOFF        0.731293
        COLLECTION     0.268707
Name: loan_status, dtype: float64
1
df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
2
df.head()
Unnamed: 0	Unnamed: 0.1	loan_status	Principal	terms	effective_date	due_date	age	education	Gender	dayofweek	weekend
0	0	0	PAIDOFF	1000	30	2016-09-08	2016-10-07	45	High School or Below	0	3	0
1	2	2	PAIDOFF	1000	30	2016-09-08	2016-10-07	33	Bechalor	1	3	0
2	3	3	PAIDOFF	1000	15	2016-09-08	2016-09-22	27	college	0	3	0
3	4	4	PAIDOFF	1000	30	2016-09-09	2016-10-08	28	college	1	4	1
4	6	6	PAIDOFF	1000	30	2016-09-09	2016-10-08	29	college	0	4	1
1
df.groupby(['education'])['loan_status'].value_counts(normalize=True)
education             loan_status
Bechalor              PAIDOFF        0.750000
                      COLLECTION     0.250000
High School or Below  PAIDOFF        0.741722
                      COLLECTION     0.258278
Master or Above       COLLECTION     0.500000
                      PAIDOFF        0.500000
college               PAIDOFF        0.765101
                      COLLECTION     0.234899
Name: loan_status, dtype: float64
1
df[['Principal','terms','age','Gender','education']].head()
Principal	terms	age	Gender	education
0	1000	30	45	0	High School or Below
1	1000	30	33	1	Bechalor
2	1000	15	27	0	college
3	1000	30	28	1	college
4	1000	30	29	0	college
1
Feature = df[['Principal','terms','age','Gender','weekend']]
2
Feature = pd.concat([Feature,pd.get_dummies(df['education'])], axis=1)
3
Feature.drop(['Master or Above'], axis = 1,inplace=True)
4
Feature.head()
Principal	terms	age	Gender	weekend	Bechalor	High School or Below	college
0	1000	30	45	0	0	0	1	0
1	1000	30	33	1	0	1	0	0
2	1000	15	27	0	0	0	0	1
3	1000	30	28	1	1	0	0	1
4	1000	30	29	0	1	0	0	1
1
X = Feature
2
X[0:5]
Principal	terms	age	Gender	weekend	Bechalor	High School or Below	college
0	1000	30	45	0	0	0	1	0
1	1000	30	33	1	0	1	0	0
2	1000	15	27	0	0	0	0	1
3	1000	30	28	1	1	0	0	1
4	1000	30	29	0	1	0	0	1
1
y = df['loan_status'].values
2
y[0:5]
array(['PAIDOFF', 'PAIDOFF', 'PAIDOFF', 'PAIDOFF', 'PAIDOFF'],
      dtype=object)
1
X= preprocessing.StandardScaler().fit(X).transform(X)
2
X[0:5]
array([[ 0.51578458,  0.92071769,  2.33152555, -0.42056004, -1.20577805,
        -0.38170062,  1.13639374, -0.86968108],
       [ 0.51578458,  0.92071769,  0.34170148,  2.37778177, -1.20577805,
         2.61985426, -0.87997669, -0.86968108],
       [ 0.51578458, -0.95911111, -0.65321055, -0.42056004, -1.20577805,
        -0.38170062, -0.87997669,  1.14984679],
       [ 0.51578458,  0.92071769, -0.48739188,  2.37778177,  0.82934003,
        -0.38170062, -0.87997669,  1.14984679],
       [ 0.51578458,  0.92071769, -0.3215732 , -0.42056004,  0.82934003,
        -0.38170062, -0.87997669,  1.14984679]])
1
import sklearn as sk
2
from sklearn.neighbors import KNeighborsClassifier
3
#from sklearn.cross_validation import train_test_split
4
#yesssss
5
from sklearn.model_selection import train_test_split
6
import sklearn.metrics as metrics
1
#from sklearn.model_selection import train_test_split
1
seed=50
2
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.40, random_state=seed)
1
score=[]
2
for k in range(1,100):
3
    knn=KNeighborsClassifier(n_neighbors=k,weights='uniform')
4
    knn.fit(X_train,y_train)
5
    predKNN=knn.predict(X_test)
6
    accuracy=metrics.accuracy_score(predKNN,y_test)
7
    score.append(accuracy*100)
8
    print (k,': ',accuracy)
1 :  0.6906474820143885
2 :  0.5827338129496403
3 :  0.7266187050359713
4 :  0.6762589928057554
5 :  0.7482014388489209
6 :  0.6762589928057554
7 :  0.7482014388489209
8 :  0.7050359712230215
9 :  0.7769784172661871
10 :  0.7338129496402878
11 :  0.7482014388489209
12 :  0.7194244604316546
13 :  0.7482014388489209
14 :  0.7266187050359713
15 :  0.7410071942446043
16 :  0.7338129496402878
17 :  0.7482014388489209
18 :  0.7266187050359713
19 :  0.7553956834532374
20 :  0.7266187050359713
21 :  0.762589928057554
22 :  0.7194244604316546
23 :  0.762589928057554
24 :  0.7482014388489209
25 :  0.762589928057554
26 :  0.762589928057554
27 :  0.762589928057554
28 :  0.762589928057554
29 :  0.762589928057554
30 :  0.762589928057554
31 :  0.7913669064748201
32 :  0.7769784172661871
33 :  0.7913669064748201
34 :  0.7913669064748201
35 :  0.7913669064748201
36 :  0.7985611510791367
37 :  0.7841726618705036
38 :  0.7913669064748201
39 :  0.7913669064748201
40 :  0.7841726618705036
41 :  0.8345323741007195
42 :  0.8345323741007195
43 :  0.8273381294964028
44 :  0.8345323741007195
45 :  0.8201438848920863
46 :  0.8201438848920863
47 :  0.8201438848920863
48 :  0.8273381294964028
49 :  0.8201438848920863
50 :  0.8201438848920863
51 :  0.8201438848920863
52 :  0.8201438848920863
53 :  0.8201438848920863
54 :  0.8201438848920863
55 :  0.8201438848920863
56 :  0.8201438848920863
57 :  0.8201438848920863
58 :  0.8201438848920863
59 :  0.8201438848920863
60 :  0.8201438848920863
61 :  0.8201438848920863
62 :  0.8201438848920863
63 :  0.8201438848920863
64 :  0.8201438848920863
65 :  0.8201438848920863
66 :  0.8201438848920863
67 :  0.8201438848920863
68 :  0.8201438848920863
69 :  0.8201438848920863
70 :  0.8201438848920863
71 :  0.8201438848920863
72 :  0.8201438848920863
73 :  0.8201438848920863
74 :  0.8201438848920863
75 :  0.8201438848920863
76 :  0.8201438848920863
77 :  0.8201438848920863
78 :  0.8201438848920863
79 :  0.8201438848920863
80 :  0.8201438848920863
81 :  0.8201438848920863
82 :  0.8201438848920863
83 :  0.8201438848920863
84 :  0.8201438848920863
85 :  0.8201438848920863
86 :  0.8201438848920863
87 :  0.8201438848920863
88 :  0.8201438848920863
89 :  0.8201438848920863
90 :  0.8201438848920863
91 :  0.8201438848920863
92 :  0.8201438848920863
93 :  0.8201438848920863
94 :  0.8201438848920863
95 :  0.8201438848920863
96 :  0.8201438848920863
97 :  0.8201438848920863
98 :  0.8201438848920863
99 :  0.8201438848920863
1
print(score.index(max(score))+1,' : ',round(max(score),2),'%')
41  :  83.45 %
1
plt.plot(range(1,100),score)
2
plt.xlabel('Number of Neighbors K')
3
plt.ylabel('Train Accuracy')
Text(0, 0.5, 'Train Accuracy')

1
knn=KNeighborsClassifier(n_neighbors=41,weights='uniform')
2
knn.fit(X_train,y_train)
3
predKNN=knn.predict(X_test)
4
accuracy=metrics.accuracy_score(predKNN,y_test)
5
print("accuracy : ",round(accuracy,3)*100,'%')
accuracy :  83.5 %
1
from sklearn.metrics import classification_report,jaccard_similarity_score,log_loss,f1_score
2
from sklearn.metrics import jaccard_score
3
#pos_label=jaccard_score(y_test, dt_yhat,pos_label = "PAIDOFF")
4
print(classification_report(y_test,predKNN))
5
print('\n')
6
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_test,predKNN)*100,2),'%')
7
print('\n')
8
print('F1-SCORE : ',f1_score(y_test,predKNN,average=None))
9
print('\n')
10
print('Train Accuracy: ',metrics.accuracy_score(y_train, knn.predict(X_train))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       0.67      0.16      0.26        25
     PAIDOFF       0.84      0.98      0.91       114

    accuracy                           0.83       139
   macro avg       0.75      0.57      0.58       139
weighted avg       0.81      0.83      0.79       139



Jaccard Similarity Score :  83.45 %


F1-SCORE :  [0.25806452 0.90688259]


Train Accuracy:  72.46376811594203 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
#from sklearn.metrics import classification_report
1
#from sklearn.metrics import classification_report,jaccard_similarity_score,log_loss,f1_score
1
from sklearn.tree import DecisionTreeClassifier
2
#from sklearn.grid_search import GridSearchCV
3
#yesssssss
4
from sklearn.model_selection import learning_curve, GridSearchCV
5
dtree=DecisionTreeClassifier()
1
parameter_grid = {'max_depth': [1, 2, 3, 4, 5,6,5,9,15,20],
2
                  'max_features': [1, 2, 3, 4,5,6,7,8],
3
                 'random_state':[0,15,20,35,50,80,100,150,180,200],
4
                 'criterion':['gini','entropy'],
5
                 }
6
​
7
grid_search = GridSearchCV(dtree, param_grid = parameter_grid,
8
                          cv =10)
9
​
10
grid_search.fit(X_train, y_train)
11
​
12
print ("Best Score: {}".format(grid_search.best_score_))
13
print ("Best params: {}".format(grid_search.best_params_))
Best Score: 0.7435714285714285
Best params: {'criterion': 'entropy', 'max_depth': 5, 'max_features': 2, 'random_state': 0}
1
dtree=DecisionTreeClassifier(max_depth=5,criterion='entropy',max_features=2,random_state=0)
1
dtree.fit(X_train,y_train)
2
pred_Dtree=dtree.predict(X_test)
1
print(classification_report(y_test,pred_Dtree))
2
print('\n')
3
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_test,pred_Dtree)*100,2),'%')
4
print('\n')
5
print('F1-SCORE : ',f1_score(y_test,pred_Dtree,average=None))
6
print('\n')
7
print('Train Accuracy: ',metrics.accuracy_score(y_train, dtree.predict(X_train))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       0.32      0.40      0.36        25
     PAIDOFF       0.86      0.82      0.84       114

    accuracy                           0.74       139
   macro avg       0.59      0.61      0.60       139
weighted avg       0.76      0.74      0.75       139



Jaccard Similarity Score :  74.1 %


F1-SCORE :  [0.35714286 0.83783784]


Train Accuracy:  77.29468599033817 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
from sklearn.svm import SVC
2
from sklearn.metrics import accuracy_score
1
svm=SVC().fit(X_train,y_train)
1
pred_svm=svm.predict(X_test)
1
​
2
print(classification_report(y_test,pred_svm))
3
print('\n')
4
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_test,pred_svm)*100,2),'%')
5
print('\n')
6
print('F1-SCORE : ',f1_score(y_test,pred_svm,average=None))
7
print('\n')
8
print('Train Accuracy: ',metrics.accuracy_score(y_train, svm.predict(X_train))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       0.33      0.24      0.28        25
     PAIDOFF       0.84      0.89      0.87       114

    accuracy                           0.78       139
   macro avg       0.59      0.57      0.57       139
weighted avg       0.75      0.78      0.76       139



Jaccard Similarity Score :  77.7 %


F1-SCORE :  [0.27906977 0.86808511]


Train Accuracy:  76.32850241545893 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
from sklearn.linear_model import LogisticRegression
1
lgm=LogisticRegression()
1
lgm.fit(X_train,y_train)
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
1
pred_lgm=lgm.predict(X_test)
1
print(classification_report(y_test,pred_lgm))
2
print('\n')
3
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_test,pred_lgm)*100,2),'%')
4
print('\n')
5
print('F1-SCORE : ',f1_score(y_test,pred_lgm,average=None))
6
print('\n')
7
print('Train Accuracy: ',metrics.accuracy_score(y_train, lgm.predict(X_train))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       0.29      0.20      0.24        25
     PAIDOFF       0.84      0.89      0.86       114

    accuracy                           0.77       139
   macro avg       0.57      0.55      0.55       139
weighted avg       0.74      0.77      0.75       139



Jaccard Similarity Score :  76.98 %


F1-SCORE :  [0.23809524 0.86440678]


Train Accuracy:  73.91304347826086 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
from sklearn.metrics import jaccard_similarity_score
2
from sklearn.metrics import f1_score
3
from sklearn.metrics import log_loss
1
test_df = pd.read_csv('dataset\loan_test.csv')
2
test_df.head()
Unnamed: 0	Unnamed: 0.1	loan_status	Principal	terms	effective_date	due_date	age	education	Gender
0	1	1	PAIDOFF	1000	30	9/8/2016	10/7/2016	50	Bechalor	female
1	5	5	PAIDOFF	300	7	9/9/2016	9/15/2016	35	Master or Above	male
2	21	21	PAIDOFF	1000	30	9/10/2016	10/9/2016	43	High School or Below	female
3	24	24	PAIDOFF	1000	30	9/10/2016	10/9/2016	26	college	male
4	35	35	PAIDOFF	800	15	9/11/2016	9/25/2016	29	Bechalor	male
1
test_df['due_date'] = pd.to_datetime(test_df['due_date'])
2
test_df['effective_date'] = pd.to_datetime(test_df['effective_date'])
3
test_df.head()
Unnamed: 0	Unnamed: 0.1	loan_status	Principal	terms	effective_date	due_date	age	education	Gender
0	1	1	PAIDOFF	1000	30	2016-09-08	2016-10-07	50	Bechalor	female
1	5	5	PAIDOFF	300	7	2016-09-09	2016-09-15	35	Master or Above	male
2	21	21	PAIDOFF	1000	30	2016-09-10	2016-10-09	43	High School or Below	female
3	24	24	PAIDOFF	1000	30	2016-09-10	2016-10-09	26	college	male
4	35	35	PAIDOFF	800	15	2016-09-11	2016-09-25	29	Bechalor	male
1
test_df['dayofweek'] = test_df['effective_date'].dt.dayofweek
2
test_df.head()
Unnamed: 0	Unnamed: 0.1	loan_status	Principal	terms	effective_date	due_date	age	education	Gender	dayofweek
0	1	1	PAIDOFF	1000	30	2016-09-08	2016-10-07	50	Bechalor	female	3
1	5	5	PAIDOFF	300	7	2016-09-09	2016-09-15	35	Master or Above	male	4
2	21	21	PAIDOFF	1000	30	2016-09-10	2016-10-09	43	High School or Below	female	5
3	24	24	PAIDOFF	1000	30	2016-09-10	2016-10-09	26	college	male	5
4	35	35	PAIDOFF	800	15	2016-09-11	2016-09-25	29	Bechalor	male	6
1
test_df['weekend']=test_df['dayofweek'].apply(lambda x: 1 if (x>3) else 0)
1
sns.heatmap(test_df.isnull())
<AxesSubplot:>

1
test_df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
1
dumm=pd.get_dummies(test_df['education'])
2
#dumm=dumm.drop('Master or Above',axis=1,inplace=True)
3
dumm=dumm[['Bechalor','High School or Below','college']]
1
test_feature = test_df[['Principal','terms','age','Gender','weekend']]
2
test_feature = pd.concat([test_feature,dumm], axis=1)
3
#test_feature.drop(['Master or Above'], axis = 1,inplace=True)
4
test_feature.head()
Principal	terms	age	Gender	weekend	Bechalor	High School or Below	college
0	1000	30	50	1	0	1	0	0
1	300	7	35	0	1	0	0	0
2	1000	30	43	1	1	0	1	0
3	1000	30	26	0	1	0	0	1
4	800	15	29	0	1	1	0	0
1
test_data= test_feature
2
test_data= preprocessing.StandardScaler().fit(test_data).transform(test_data)
1
y_t=test_df['loan_status']
1
knn=KNeighborsClassifier()
2
knn.fit(X,y)
3
predKNN_test=knn.predict(test_data)
4
accuracy=metrics.accuracy_score(predKNN_test,y_t)
5
print("accuracy : ",round(accuracy,3)*100,'%')
accuracy :  74.1 %
1
score=[]
2
for k in range(1,100):
3
    knn=KNeighborsClassifier(n_neighbors=k,weights='uniform')
4
    knn.fit(X,y)
5
    predKNN=knn.predict(test_data)
6
    accuracy=metrics.accuracy_score(predKNN,y_t)
7
    score.append(accuracy*100)
8
    print (k,': ',accuracy)
1 :  0.7037037037037037
2 :  0.5740740740740741
3 :  0.6481481481481481
4 :  0.6296296296296297
5 :  0.7407407407407407
6 :  0.6851851851851852
7 :  0.7222222222222222
8 :  0.7037037037037037
9 :  0.7037037037037037
10 :  0.6851851851851852
11 :  0.6851851851851852
12 :  0.6666666666666666
13 :  0.7037037037037037
14 :  0.7037037037037037
15 :  0.7222222222222222
16 :  0.7037037037037037
17 :  0.7222222222222222
18 :  0.7037037037037037
19 :  0.7222222222222222
20 :  0.7407407407407407
21 :  0.7592592592592593
22 :  0.7592592592592593
23 :  0.7592592592592593
24 :  0.7222222222222222
25 :  0.7407407407407407
26 :  0.7777777777777778
27 :  0.7592592592592593
28 :  0.7777777777777778
29 :  0.7592592592592593
30 :  0.7777777777777778
31 :  0.7407407407407407
32 :  0.7962962962962963
33 :  0.7777777777777778
34 :  0.7962962962962963
35 :  0.7962962962962963
36 :  0.7777777777777778
37 :  0.7962962962962963
38 :  0.7962962962962963
39 :  0.7962962962962963
40 :  0.7962962962962963
41 :  0.7962962962962963
42 :  0.7962962962962963
43 :  0.7777777777777778
44 :  0.7962962962962963
45 :  0.7962962962962963
46 :  0.7962962962962963
47 :  0.7777777777777778
48 :  0.7777777777777778
49 :  0.7592592592592593
50 :  0.7777777777777778
51 :  0.7777777777777778
52 :  0.7777777777777778
53 :  0.7407407407407407
54 :  0.7407407407407407
55 :  0.7407407407407407
56 :  0.7407407407407407
57 :  0.7407407407407407
58 :  0.7407407407407407
59 :  0.7407407407407407
60 :  0.7407407407407407
61 :  0.7407407407407407
62 :  0.7407407407407407
63 :  0.7407407407407407
64 :  0.7407407407407407
65 :  0.7407407407407407
66 :  0.7407407407407407
67 :  0.7407407407407407
68 :  0.7407407407407407
69 :  0.7407407407407407
70 :  0.7407407407407407
71 :  0.7407407407407407
72 :  0.7407407407407407
73 :  0.7407407407407407
74 :  0.7407407407407407
75 :  0.7407407407407407
76 :  0.7407407407407407
77 :  0.7407407407407407
78 :  0.7407407407407407
79 :  0.7407407407407407
80 :  0.7407407407407407
81 :  0.7407407407407407
82 :  0.7407407407407407
83 :  0.7407407407407407
84 :  0.7407407407407407
85 :  0.7407407407407407
86 :  0.7407407407407407
87 :  0.7407407407407407
88 :  0.7407407407407407
89 :  0.7407407407407407
90 :  0.7407407407407407
91 :  0.7407407407407407
92 :  0.7407407407407407
93 :  0.7407407407407407
94 :  0.7407407407407407
95 :  0.7407407407407407
96 :  0.7407407407407407
97 :  0.7407407407407407
98 :  0.7407407407407407
99 :  0.7407407407407407
1
​
2
print(score.index(max(score))+1,' : ',round(max(score),2),'%')
32  :  79.63 %
1
knn=KNeighborsClassifier(n_neighbors=32)
2
knn.fit(X,y)
3
predKNN_test=knn.predict(test_data)
4
accuracy=metrics.accuracy_score(predKNN_test,y_t)
5
print("accuracy : ",round(accuracy,3)*100,'%')
accuracy :  79.60000000000001 %
1
print(classification_report(y_t,predKNN_test))
2
print('\n')
3
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_t,predKNN_test)*100,2),'%')
4
print('\n')
5
print('F1-SCORE : ',f1_score(y_t,predKNN_test,average=None))
6
print('\n')
7
print('Train Accuracy: ',metrics.accuracy_score(y,knn.predict(X))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       0.71      0.36      0.48        14
     PAIDOFF       0.81      0.95      0.87        40

    accuracy                           0.80        54
   macro avg       0.76      0.65      0.67        54
weighted avg       0.78      0.80      0.77        54



Jaccard Similarity Score :  79.63 %


F1-SCORE :  [0.47619048 0.87356322]


Train Accuracy:  74.85549132947978 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
parameter_grid = {'max_depth': [1, 2, 3, 4, 5,6,5,9,15,20],
2
                  'max_features': [1, 2, 3, 4,5,6,7,8],
3
                 'random_state':[0,15,20,35,50,80,100,150,180,200],
4
                 'criterion':['gini','entropy'],
5
                 }
6
​
7
grid_search = GridSearchCV(dtree, param_grid = parameter_grid,
8
                          cv =10)
9
​
10
grid_search.fit(X, y)
11
​
12
print ("Best Score: {}".format(grid_search.best_score_))
13
print ("Best params: {}".format(grid_search.best_params_))
Best Score: 0.7695798319327731
Best params: {'criterion': 'entropy', 'max_depth': 6, 'max_features': 4, 'random_state': 20}
1
dtree=DecisionTreeClassifier(max_depth=6,criterion='entropy',max_features=4,random_state=20).fit(X,y)
2
pred_Dtree=dtree.predict(test_data)
1
print(classification_report(y_t,pred_Dtree))
2
print('\n')
3
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_t,pred_Dtree)*100,2),'%')
4
print('\n')
5
print('F1-SCORE : ',f1_score(y_t,pred_Dtree,average=None))
6
print('\n')
7
print('Train Accuracy: ',metrics.accuracy_score(y, dtree.predict(X))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       0.44      0.29      0.35        14
     PAIDOFF       0.78      0.88      0.82        40

    accuracy                           0.72        54
   macro avg       0.61      0.58      0.59        54
weighted avg       0.69      0.72      0.70        54



Jaccard Similarity Score :  72.22 %


F1-SCORE :  [0.34782609 0.82352941]


Train Accuracy:  79.47976878612717 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
svm=SVC().fit(X,y)
2
pred_svm=svm.predict(test_data)
1
print(classification_report(y_t,pred_svm))
2
print('\n')
3
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_t,pred_svm)*100,2),'%')
4
print('\n')
5
print('F1-SCORE : ',f1_score(y_t,pred_svm,average=None))
6
print('\n')
7
print('Train Accuracy: ',metrics.accuracy_score(y, svm.predict(X))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       0.00      0.00      0.00        14
     PAIDOFF       0.74      0.97      0.84        40

    accuracy                           0.72        54
   macro avg       0.37      0.49      0.42        54
weighted avg       0.55      0.72      0.62        54



Jaccard Similarity Score :  72.22 %


F1-SCORE :  [0.         0.83870968]


Train Accuracy:  76.01156069364163 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
lgm=LogisticRegression().fit(X,y)
1
pred_lgm=lgm.predict(test_data)
1
print(classification_report(y_t,pred_lgm))
2
print('\n')
3
print('Jaccard Similarity Score : ',round(jaccard_similarity_score(y_t,pred_lgm)*100,2),'%')
4
print('\n')
5
print('F1-SCORE : ',f1_score(y_t,pred_lgm,average=None))
6
print('\n')
7
print('Train Accuracy: ',metrics.accuracy_score(y, lgm.predict(X))*100,'%')
              precision    recall  f1-score   support

  COLLECTION       1.00      0.07      0.13        14
     PAIDOFF       0.75      1.00      0.86        40

    accuracy                           0.76        54
   macro avg       0.88      0.54      0.50        54
weighted avg       0.82      0.76      0.67        54



Jaccard Similarity Score :  75.93 %


F1-SCORE :  [0.13333333 0.86021505]


Train Accuracy:  75.43352601156069 %
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
Algoritm=['KNN','Decision Tree','SVM','LogisticRegression']
2
j_knn=round(jaccard_similarity_score(y_t,predKNN_test)*100,2)
3
j_dtree=round(jaccard_similarity_score(y_t,pred_Dtree)*100,2)
4
j_svm=round(jaccard_similarity_score(y_t,pred_svm)*100,2)
5
j_lgm=round(jaccard_similarity_score(y_t,pred_lgm)*100,2)
6
Jaccard=[j_knn,j_dtree,j_svm,j_lgm]
7
​
8
f1_knn=f1_score(y_t,predKNN_test,average=None)
9
f1_dtree=f1_score(y_t,pred_Dtree,average=None)
10
f1_svm=f1_score(y_t,pred_svm,average=None)
11
f1_lgm=f1_score(y_t,pred_lgm,average=None)
12
F1_score=[f1_knn,f1_dtree,f1_svm,f1_lgm]
C:\Users\Noosi\AppData\Roaming\Python\Python38\site-packages\sklearn\metrics\_classification.py:656: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.
  warnings.warn('jaccard_similarity_score has been deprecated and replaced '
1
table = pd.DataFrame({
2
    "Algorithm": Algoritm,
3
    "Jaccatd": Jaccard,
4
    "F1-Score": F1_score,
5
    "LogLoss":[np.NAN,np.NAN,np.NAN,np.NAN]})
1
table
Algorithm	Jaccatd	F1-Score	LogLoss
0	KNN	79.63	[0.4761904761904762, 0.8735632183908046]	NaN
1	Decision Tree	72.22	[0.34782608695652173, 0.823529411764706]	NaN
2	SVM	72.22	[0.0, 0.8387096774193549]	NaN
3	LogisticRegression	75.93	[0.13333333333333333, 0.8602150537634409]	NaN
1
#khaste nabashid noosi!!!!!
2
​
